\section{Find the optimal solutions of a set}

\paragraph{Algorithm} One of the most well-known algorithms to find the set of optimal solutions was first proposed by Kung in 1975 \cite{kung_optima}, and even though it was presented as an algorithm to find the maxima of a set, it can be applied to find both the maxima and minima by using the definition of dominance. The methodology is to divide the problem into several sub-problems in the lower dimensional space --- allowing the algorithm to be applied to any number of dimensions. The algorithm achieves that by pre-sorting the solutions by one of the dimensions. That way, a solution is optimal only if its projection onto the other dimensions is also optimal --- given that the previous solutions in the sorting list dominate the current solution in the dimension by which the sorting is performed.

\input{algorithms/optima}

\paragraph{Three-dimensional implementation} The first step of the algorithm consists of sorting the solutions by the following order: $\{x_1,x_2,x_3\}$ --- any sorting algorithm with logarithmic time complexity is a valid choice. Given the order, the solutions projection contains the second and third dimensions --- $x_2$ and $x_3$. The more complex operations are the optimality verification and the update of the lower dimensional space --- lines 8 and 10 of Algorithm \ref{alg:optima}. This can be achieved by using a balanced binary tree, called \textit{projection tree}, that stores its elements in the following order: $\{x_2,x_3\}$. That way, verifying if a solution is optimal or not consists of inserting the solution in the projection tree and comparing it with its predecessor in the tree. Since the predecessor was inserted first in the projection tree and the fact that it is the predecessor implies that it has an equal or lower value of $x_1$ and $x_2$, respectively, than the current solution. Thus, the current solution is optimal only if it has a lower $x_3$ value than its predecessor. In the case that the solution is optimal, the remaining operation is to update the projection tree, which can be done by iterating through the successors in the tree, removing them in the process, until one that has a lower $x_3$ value is reached.


\paragraph{Time complexity} Given the initial sorting of the solutions by a particular dimension, the algorithm starts with $\mathcal{O}(n\log n)$ time complexity. Also, each point is inserted in the projection tree even before verifying its optimality, which adds another $\mathcal{O}(n\log n)$ to the overall complexity. Verifying a solution's optimality implies accessing the predecessor in the projection tree, adding $\mathcal{O}(n)$ --- given that the binary tree is threaded; otherwise, $\mathcal{O}(n\log n)$. Finally, if the solution is not optimal, it must be removed from the tree. Otherwise, the now dominated projected solutions must be removed from the tree --- these are the successors, accessible in $\mathcal{O}(1)$ time each. However, it is known that a solution is inserted only once in the tree, and therefore, removed at most once, either in the first or in the second scenario --- it can stay in the projection tree until the end, and not be removed. Therefore, the complexity increases another $\mathcal{O}(n\log n)$, totalling a complexity of $\mathcal{O}(3\times n\log n + n) = \underline{\smash{\mathcal{O}(n\log n)}}$.

\paragraph{Space complexity} Beyond the set that stores the solutions, the only auxiliary storage necessary is the projection tree, containing at most the total number of solutions in the set --- in the case they are all optimal. Therefore, the space complexity of the algorithm is $\underline{\smash{\mathcal{O}(n)}}$. 